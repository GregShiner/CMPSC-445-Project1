{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install dependencies and download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt\n",
    "#!curl -l -o data.csv \"https://phl.carto.com/api/v2/sql?q=SELECT+*,+ST_Y(the_geom)+AS+lat,+ST_X(the_geom)+AS+lng+FROM+opa_properties_public&filename=opa_properties_public&format=csv&skipfields=cartodb_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see all of the columns available in the dataset. Many of these columns are not really necesary. For example, geographical data such as street addresses are not very useful because they don't contain any categorical or numerical data that can be easily consumed by a linear regression mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "COLUMNS_TO_REMOVE = \"\"\"\n",
    "the_geom\n",
    "the_geom_webmercator\n",
    "beginning_point\n",
    "book_and_page\n",
    "building_code_description\n",
    "category_code_description\n",
    "cross_reference\n",
    "geographic_ward\n",
    "house_number\n",
    "location\n",
    "mailing_address_1\n",
    "mailing_address_2\n",
    "mailing_care_of\n",
    "mailing_city_state\n",
    "mailing_street\n",
    "mailing_zip\n",
    "other_building\n",
    "owner_1\n",
    "owner_2\n",
    "parcel_number\n",
    "registry_number\n",
    "state_code\n",
    "street_code\n",
    "street_designation\n",
    "street_direction\n",
    "street_name\n",
    "suffix\n",
    "assessment_date\n",
    "recording_date\n",
    "sale_date\n",
    "pin\n",
    "zip_code\n",
    "parcel_shape\n",
    "quality_grade\n",
    "building_code\n",
    "building_code_new\n",
    "building_code_description_new\n",
    "objectid\n",
    "unit\n",
    "\"\"\"\n",
    "df_first_column_drop = df.drop(columns=COLUMNS_TO_REMOVE.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df_first_column_drop.to_csv('data_first_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note about this dataset is that this data is for all types of properties in Philadelphia. Since we are only interested in housing data, we will filter this data to only be for housing type properties. This is accomplished by filtering for properties with a category code of 1 (single family), 2 (multi family), or 3 (mixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Get all unique values of (category_code, category_code_description)\n",
    "df[['category_code', 'category_code_description']].drop_duplicates().sort_values(by='category_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Print number of data points\n",
    "print(df_first_column_drop.shape)\n",
    "# Filter category-type to 1, or 2\n",
    "df_first_column_drop = df_first_column_drop[df_first_column_drop['category_code'] <= 2]\n",
    "# Print number of data points\n",
    "print(df_first_column_drop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed some of the columns, lets see what we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df_first_column_drop.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a lot of columns. It is very likely that many more of these will get dropped for the following reasons:\n",
    "1. Too many missing values\n",
    "2. Too many unique values\n",
    "3. Not enough correlation with the target variable\n",
    "\n",
    "Let's start by dealing with the first case: too many missing values. We will check this by counting the number of missing values in each column and sorting by this number.\n",
    "\n",
    "There are also a few columns that seem to have a default value put in them instead of being left blank. Lets also change these default values to na, so they can be counted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df_first_column_drop['depth'] = df_first_column_drop['depth'].replace(0, np.nan)\n",
    "df_first_column_drop['total_area'] = df_first_column_drop['total_area'].replace(0, np.nan)\n",
    "df_first_column_drop['total_livable_area'] = df_first_column_drop['total_livable_area'].replace(0, np.nan)\n",
    "df_first_column_drop['year_built'] = df_first_column_drop['year_built'].replace(0, np.nan)\n",
    "df_first_column_drop['sale_price'] = df_first_column_drop['sale_price'].replace(1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We still have a lot of columns. It is very likely that many more of these will get dropped for the following reasons:\n",
    "1. Too many missing values\n",
    "2. Too many unique values\n",
    "3. Not enough correlation with the target variable\n",
    "\n",
    "Let's start by dealing withe the first case: too many missing values. We will check this by counting the number of missing values in each column and sorting by this number.\n",
    "\"\"\"\n",
    "#print(df_first_column_drop.isna().sum().sort_values(ascending=False))\n",
    "# Print the number of missing values and the percentage of missing values\n",
    "missing_values = df_first_column_drop.isna().sum().sort_values(ascending=False)\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "missing_values = pd.DataFrame(missing_values, columns=['missing_values'])\n",
    "missing_values['percentage_missing'] = missing_values['missing_values'] / len(df_first_column_drop)\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some of these pieces of missing data won't be that big of a deal because we can either fill or impute the data. We can also drop rows with missing data, but we should do this sparingly. For some categorical columns, you could set a default value, but I will not be doing this much. You cannot assume exactly what the person who entered the data intended by leaving it blank, and filling it could cause innacuracies, especially with columns with a lot of missing data. But for some columns, there is simply too much missing data. For this reason, I will be dropping all columns with more than 25% missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "COLUMNS_TO_REMOVE_MISSING_VALUES = missing_values[missing_values['percentage_missing'] > 0.25].index\n",
    "df_second_column_drop = df_first_column_drop.drop(columns=COLUMNS_TO_REMOVE_MISSING_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "df_second_column_drop.to_csv('data_second_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's recheck the list of columns with missing data, and address each one indiviually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "missing_values_2 = df_second_column_drop.isna().sum().sort_values(ascending=False)\n",
    "missing_values_2 = missing_values_2[missing_values_2 > 0]\n",
    "missing_values_2 = pd.DataFrame(missing_values_2, columns=['missing_values'])\n",
    "missing_values_2['percentage_missing'] = missing_values_2['missing_values'] / len(df_second_column_drop)\n",
    "print(missing_values_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are left with columns with most of their data filled in, it is much safter to start filling in with default or imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_fill = df_second_column_drop\n",
    "# year_built_estimate can be filled with N's\n",
    "df_fill['year_built_estimate'] = df_fill['year_built_estimate'].fillna('N')\n",
    "# garage_spaces can just be filled with 0. It is a fair assumption that if the value is missing, there is no garage\n",
    "df_fill['garage_spaces'] = df_fill['garage_spaces'].fillna(0)\n",
    "# fireplaces can just be filled with 0. It is a fair assumption that if the value is missing, there are no fireplaces\n",
    "df_fill['fireplaces'] = df_fill['fireplaces'].fillna(0)\n",
    "# number of bathrooms can be filled with the median\n",
    "df_fill['number_of_bathrooms'] = median_imputer.fit_transform(df_fill[['number_of_bathrooms']])\n",
    "# interior condition can be filled with the median\n",
    "df_fill['interior_condition'] = median_imputer.fit_transform(df_fill[['interior_condition']])\n",
    "# exterior condition can be filled with the median\n",
    "df_fill['exterior_condition'] = median_imputer.fit_transform(df_fill[['exterior_condition']])\n",
    "# number of bedrooms can be filled with the median\n",
    "df_fill['number_of_bedrooms'] = median_imputer.fit_transform(df_fill[['number_of_bedrooms']])\n",
    "# number of stories can be filled with the median\n",
    "df_fill['number_stories'] = median_imputer.fit_transform(df_fill[['number_stories']])\n",
    "# NOTE: Maybe change these to mode\n",
    "# general construction can be filled with a new category called 'unknown'\n",
    "df_fill['general_construction'] = df_fill['general_construction'].fillna('unknown')\n",
    "# quality grade will be skipped for now because it needs to be transformed into a numeric column\n",
    "# year built can be filled with the median\n",
    "df_fill['year_built'] = median_imputer.fit_transform(df_fill[['year_built']])\n",
    "# total livable area can be filled with the median\n",
    "df_fill['total_livable_area'] = median_imputer.fit_transform(df_fill[['total_livable_area']])\n",
    "# topography can be filled with a new category called 'unknown'\n",
    "df_fill['topography'] = df_fill['topography'].fillna('unknown')\n",
    "# depth can be filled with the median\n",
    "df_fill['depth'] = median_imputer.fit_transform(df_fill[['depth']])\n",
    "# total area can be filled with the median\n",
    "df_fill['total_area'] = median_imputer.fit_transform(df_fill[['total_area']])\n",
    "# view type can be filled with a new category called 'unknown'\n",
    "df_fill['view_type'] = df_fill['view_type'].fillna('unknown')\n",
    "# off street open can be filled with the median\n",
    "df_fill['off_street_open'] = median_imputer.fit_transform(df_fill[['off_street_open']])\n",
    "# frontage can be filled with the median\n",
    "df_fill['frontage'] = median_imputer.fit_transform(df_fill[['frontage']])\n",
    "# zoning can be filled with a new category called 'unknown'\n",
    "df_fill['zoning'] = df_fill['zoning'].fillna('unknown')\n",
    "# census tract can be filled with the median\n",
    "df_fill['census_tract'] = median_imputer.fit_transform(df_fill[['census_tract']])\n",
    "# lat and lng can be filled with the median\n",
    "df_fill['lat'] = median_imputer.fit_transform(df_fill[['lat']])\n",
    "df_fill['lng'] = median_imputer.fit_transform(df_fill[['lng']])\n",
    "# taxable building can be filled with the median\n",
    "df_fill['taxable_building'] = median_imputer.fit_transform(df_fill[['taxable_building']])\n",
    "# exempt land can be filled with the median\n",
    "df_fill['exempt_land'] = median_imputer.fit_transform(df_fill[['exempt_land']])\n",
    "# exempt building can be filled with the median\n",
    "df_fill['exempt_building'] = median_imputer.fit_transform(df_fill[['exempt_building']])\n",
    "# taxable land can be filled with the median\n",
    "df_fill['taxable_land'] = median_imputer.fit_transform(df_fill[['taxable_land']])\n",
    "# NOTE: Maybe change this to drop\n",
    "# market value can be filled with the median\n",
    "df_fill['market_value'] = median_imputer.fit_transform(df_fill[['market_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check again our missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "missing_values_3 = df_fill.isna().sum().sort_values(ascending=False)\n",
    "missing_values_3 = missing_values_3[missing_values_3 > 0]\n",
    "missing_values_3 = pd.DataFrame(missing_values_3, columns=['missing_values'])\n",
    "missing_values_3['percentage_missing'] = missing_values_3['missing_values'] / len(df_second_column_drop)\n",
    "print(missing_values_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "df_fill.to_csv('data_filled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# One hot columns\n",
    "ONE_HOT_COLUMNS = [\n",
    "    'category_code',\n",
    "    'general_construction',\n",
    "    'topography',\n",
    "    'view_type',\n",
    "    'zoning',\n",
    "]\n",
    "\n",
    "BINARY_COLUMNS = [\n",
    "    'year_built_estimate',\n",
    "    'homestead_exemption',\n",
    "    'exempt_building',\n",
    "    'exempt_land'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# One hot encode the columns\n",
    "df_one_hot = pd.get_dummies(df_fill, columns=ONE_HOT_COLUMNS)\n",
    "# Binary encode the columns\n",
    "df_one_hot['year_built_estimate'] = df_one_hot['year_built_estimate'].map({'Y': True, 'N': False})\n",
    "# Fillna with False\n",
    "df_one_hot['year_built_estimate'] = df_one_hot['year_built_estimate'].fillna(False)\n",
    "df_one_hot['homestead_exemption'] = df_one_hot['homestead_exemption'].map({80000: True, 0: False})\n",
    "# Fillna with False\n",
    "df_one_hot['homestead_exemption'] = df_one_hot['homestead_exemption'].fillna(False)\n",
    "# Exempt building should be false if 0, else true\n",
    "df_one_hot['exempt_building'] = df_one_hot['exempt_building'].map({0: False})\n",
    "# Fillna with True\n",
    "df_one_hot['exempt_building'] = df_one_hot['exempt_building'].fillna(True)\n",
    "# Exempt land should be false if 0, else true\n",
    "df_one_hot['exempt_land'] = df_one_hot['exempt_land'].map({0.0: False})\n",
    "# Fillna with True\n",
    "df_one_hot['exempt_land'] = df_one_hot['exempt_land'].fillna(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Check that all columns are numeric\n",
    "df_one_hot.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of boolean columns. We can probably drop some of these columns, but we will do that later. For now, we will just convert these columns to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Get columns with type bool\n",
    "bool_columns = df_one_hot.select_dtypes(include=bool).columns\n",
    "# Change type of columns to int\n",
    "df_one_hot[bool_columns] = df_one_hot[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Delete outliers\n",
    "df_outliers = df_one_hot\n",
    "# Delete census_tract outliers\n",
    "#df_outliers = df_outliers[df_outliers['census_tract'] < 500]\n",
    "# Delete depth outliers < 200 and > 32\n",
    "df_outliers = df_outliers[df_outliers['depth'] < 144]\n",
    "df_outliers = df_outliers[df_outliers['depth'] > 32]\n",
    "# Fireplace < 6\n",
    "df_outliers = df_outliers[df_outliers['fireplaces'] < 6]\n",
    "# Frontage < 140\n",
    "df_outliers = df_outliers[df_outliers['frontage'] < 50]\n",
    "# Garage spaces < 5\n",
    "df_outliers = df_outliers[df_outliers['garage_spaces'] < 5]\n",
    "# Market value < 10_000_000\n",
    "df_outliers = df_outliers[df_outliers['market_value'] < 2_000_000]\n",
    "# Number of bathrooms < 6\n",
    "df_outliers = df_outliers[df_outliers['number_of_bathrooms'] < 6]\n",
    "# Number of bedrooms < 15\n",
    "df_outliers = df_outliers[df_outliers['number_of_bedrooms'] < 6]\n",
    "# Number of stories < 6\n",
    "df_outliers = df_outliers[df_outliers['number_stories'] < 6]\n",
    "# Taxable building < 4_000_000\n",
    "df_outliers = df_outliers[df_outliers['taxable_building'] < 1_000_000]\n",
    "# Taxable land < 1_000_000\n",
    "df_outliers = df_outliers[df_outliers['taxable_land'] < 200_000]\n",
    "# Total area < 250_000\n",
    "df_outliers = df_outliers[df_outliers['total_area'] < 16_000]\n",
    "# Total livable area < 250_000\n",
    "df_outliers = df_outliers[df_outliers['total_livable_area'] < 8_000]\n",
    "# Year built > 1840\n",
    "df_outliers = df_outliers[df_outliers['year_built'] > 1890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Export to CSV excluding columns starting with ONE_HOT_COLUMNS\n",
    "df_out = df_outliers\n",
    "for column in ONE_HOT_COLUMNS:\n",
    "    df_out = df_out.loc[:, ~df_out.columns.str.startswith(column)]\n",
    "df_out.to_csv('data_outliers.csv', index=False)\n",
    "\n",
    "#df_outliers.to_csv('data_outliers.csv', index=False)\n",
    "df_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Create a correlation graphic agains the market value\n",
    "correlation = df_out.corr()\n",
    "# Print the correlation with the target variable\n",
    "correlation['market_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that preprocessing is complete, we can move on to feature engineering. For this step we will be applying PCA to the data. With PCA, we will be having it reduce the dimension so that we retain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the Data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_outliers), columns=df_outliers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "X = df_scaled.drop(columns='market_value')\n",
    "y = df_scaled['market_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "PCA_VARIANCE = 0.8\n",
    "# Create a PCA instance\n",
    "pca = PCA(PCA_VARIANCE)\n",
    "pca.fit(X)\n",
    "# Transform the data\n",
    "X_pca = pca.transform(X)\n",
    "print(X.shape)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "12"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "pca_model = LinearRegression()\n",
    "pca_model.fit(X_pca_train, y_pca_train)\n",
    "pca_model.score(X_pca_test, y_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a decision tree model\n",
    "tree_model = DecisionTreeRegressor()\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a random forest model\n",
    "forest_model = RandomForestRegressor()\n",
    "forest_model.fit(X_train, y_train)\n",
    "forest_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a decision tree model with PCA\n",
    "pca_tree_model = DecisionTreeRegressor()\n",
    "pca_tree_model.fit(X_pca_train, y_pca_train)\n",
    "pca_tree_model.score(X_pca_test, y_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a random forest model with PCA\n",
    "# pca_forest_model = RandomForestRegressor()\n",
    "# pca_forest_model.fit(X_pca_train, y_pca_train)\n",
    "# pca_forest_model.score(X_pca_test, y_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a gradient boosting model\n",
    "gradient_model = GradientBoostingRegressor()\n",
    "gradient_model.fit(X_train, y_train)\n",
    "gradient_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a gradient boosting model with PCA\n",
    "pca_gradient_model = GradientBoostingRegressor()\n",
    "pca_gradient_model.fit(X_pca_train, y_pca_train)\n",
    "pca_gradient_model.score(X_pca_test, y_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a model with xgboost\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a model with xgboost with PCA\n",
    "pca_xgb_model = xgb.XGBRegressor()\n",
    "pca_xgb_model.fit(X_pca_train, y_pca_train)\n",
    "pca_xgb_model.score(X_pca_test, y_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Save all of the models\n",
    "import pickle\n",
    "pickle.dump(model, open('linear_model.pkl', 'wb'))\n",
    "pickle.dump(tree_model, open('tree_model.pkl', 'wb'))\n",
    "pickle.dump(forest_model, open('forest_model.pkl', 'wb'))\n",
    "pickle.dump(gradient_model, open('gradient_model.pkl', 'wb'))\n",
    "pickle.dump(xgb_model, open('xgb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pca_model, open(f'pca_{PCA_VARIANCE}_linear_model.pkl', 'wb'))\n",
    "pickle.dump(pca_tree_model, open(f'pca_{PCA_VARIANCE}_tree_model.pkl', 'wb'))\n",
    "pickle.dump(pca_forest_model, open(f'pca_{PCA_VARIANCE}_forest_model.pkl', 'wb'))\n",
    "pickle.dump(pca_gradient_model, open(f'pca_{PCA_VARIANCE}_gradient_model.pkl', 'wb'))\n",
    "pickle.dump(pca_xgb_model, open(f'pca_{PCA_VARIANCE}_xgb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "logging.basicConfig(filename='pca.log', level=logging.INFO, format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "scores = pd.DataFrame(columns=[\"variance\", \"linear\", \"tree\", \"forest\", \"gradient\", \"xgb\"])\n",
    "# scores: variance, linear, tree, forest, gradient, xgb\n",
    "scores.set_index(\"variance\", inplace=True)\n",
    "for n in range(10, 11):\n",
    "    variance = n / 10\n",
    "    logging.info(f\"{variance=}\")\n",
    "    scores.loc[variance, \"variance\"] = variance\n",
    "    pca = PCA(variance)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    X_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "    pca_model = LinearRegression()\n",
    "    pca_model.fit(X_pca_train, y_pca_train)\n",
    "    logging.info(f\"{pca_model.score(X_pca_test, y_pca_test)=}\")\n",
    "    scores.loc[variance, \"linear\"] = pca_model.score(X_pca_test, y_pca_test)\n",
    "    pca_tree_model = DecisionTreeRegressor()\n",
    "    pca_tree_model.fit(X_pca_train, y_pca_train)\n",
    "    logging.info(f\"{pca_tree_model.score(X_pca_test, y_pca_test)=}\")\n",
    "    scores.loc[variance, \"tree\"] = pca_tree_model.score(X_pca_test, y_pca_test)\n",
    "    pca_forest_model = RandomForestRegressor()\n",
    "    pca_forest_model.fit(X_pca_train, y_pca_train)\n",
    "    logging.info(f\"{pca_forest_model.score(X_pca_test, y_pca_test)=}\")\n",
    "    scores.loc[variance, \"forest\"] = pca_forest_model.score(X_pca_test, y_pca_test)\n",
    "    pca_gradient_model = GradientBoostingRegressor()\n",
    "    pca_gradient_model.fit(X_pca_train, y_pca_train)\n",
    "    logging.info(f\"{pca_gradient_model.score(X_pca_test, y_pca_test)=}\")\n",
    "    scores.loc[variance, \"gradient\"] = pca_gradient_model.score(X_pca_test, y_pca_test)\n",
    "    pca_xgb_model = xgb.XGBRegressor()\n",
    "    pca_xgb_model.fit(X_pca_train, y_pca_train)\n",
    "    logging.info(f\"{pca_xgb_model.score(X_pca_test, y_pca_test)=}\")\n",
    "    scores.loc[variance, \"xgb\"] = pca_xgb_model.score(X_pca_test, y_pca_test)\n",
    "    scores.to_csv(\"scores.csv\", index=\"variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc6db4e0d53691edb177bb7f02452d6efa438b9e20a1af03e9281c2ab8183f1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
